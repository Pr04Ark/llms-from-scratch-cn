{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780ec141",
   "metadata": {},
   "source": [
    "# 5.4 在 PyTorch 中加载和保存模型权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de22098",
   "metadata": {},
   "source": [
    "我们讨论了如何从数值上评估训练进度以及如何从零开始预训练一个大型语言模型（LLM）。\n",
    "尽管大语言模型和数据集都相对较小，但此练习表明预训练 大语言模型的计算成本很高。\n",
    "因此，能够保存大语言模型非常重要，这样我们就不必每次想要在新会话中使用它时都重新运行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ff69a",
   "metadata": {},
   "source": [
    "如图 5.16 中的章节概述所示，我们将在本节中介绍如何保存和加载预训练模型。\n",
    "然后，在接下来的部分中，我们将从 OpenAI 加载一个功能更强大的预训练 GPT 模型到我们的 GPTModel 实例中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b4fa2d",
   "metadata": {},
   "source": [
    "**图 5.16 在训练和检查模型之后，保存模型通常很有帮助的，这样我们以后可以使用或继续训练它，这是本节的主题，然后我们将在本章的最后一节中从 OpenAI 加载预训练的模型权重**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a921a88",
   "metadata": {},
   "source": [
    "![fig5.16](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-5-16.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41081dbb",
   "metadata": {},
   "source": [
    "幸运的是，保存一个PyTorch 模型相对简单。\n",
    "这里推荐的方法是使用 torch.save 函数保存模型的所谓状态字典 state_dict，这是一个将每个层映射到其参数的字典， 代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d1411d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GPTModel' from 'transformers' (C:\\Users\\Pr04ArK\\.conda\\envs\\cell\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTModel\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFeedForward\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'GPTModel' from 'transformers' (C:\\Users\\Pr04ArK\\.conda\\envs\\cell\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPTModel\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"] * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(cfg[\"emb_dim\"] * 4, cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads #A\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) #B\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "        'mask',\n",
    "         torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x) #C\n",
    "        queries = self.W_query(x) #C\n",
    "        values = self.W_value(x) #C\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)#D\n",
    "\n",
    "        keys = keys.transpose(1, 2) #E\n",
    "        queries = queries.transpose(1, 2) #E\n",
    "        values = values.transpose(1, 2) #E\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3) #F\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) #I\n",
    "        #J\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) #K\n",
    "        return context_vec\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77373a01",
   "metadata": {},
   "source": [
    "在上面的代码中，“model.pth”是保存state_dict的文件名。 \n",
    "从技术上讲我们可以使用任何文件扩展名，但.pth扩展名是PyTorch文件的常规约定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf1918",
   "metadata": {},
   "source": [
    "然后，通过 state_dict 保存模型权重后，我们可以按照以下代码将模型权重加载到新的 GPTModel 模型实例中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d502a45",
   "metadata": {},
   "source": [
    "正如第 4 章所讨论的，dropout通过在训练期间随机“丢弃”层的神经元来帮助防止模型对训练数据过拟合。\n",
    "然而，在推理过程中，我们不想随机丢弃网络学到的任何信息。\n",
    "使用 model.eval() 能够将模型切换到评估模式进行推理，禁用模型的 dropout 层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5cd12",
   "metadata": {},
   "source": [
    "如果我们计划稍后继续预训练模型，例如使用本章早些时候定义的train_model_simple函数，建议同时保存优化器的状态。\n",
    "这样做可以在重新开始训练时保持优化器的所有参数和状态，从而更有效地继续训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6973802e",
   "metadata": {},
   "source": [
    "AdamW 等自适应优化器，会为每个模型权重存储附加参数。\n",
    "AdamW 利用历史数据动态调整每个模型参数的学习率。\n",
    "如果没有它，优化器将重置，模型可能学习效果不佳，甚至无法正确收敛，这意味着它将失去生成连贯文本的能力。\n",
    "使用torch.save，我们可以按照以下方式保存模型和优化器state_dict内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f84fd7",
   "metadata": {},
   "source": [
    "接着，我们可以通过首先使用torch.load加载保存的数据，然后使用load_state_dict方法来恢复模型和优化器的状态，具体操作如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ebb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476721c3",
   "metadata": {},
   "source": [
    "### 练习5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d6847",
   "metadata": {},
   "source": [
    "在保存权重后，在一个新的Python会话或Jupyter笔记本文件中加载模型和优化器，并使用train_model_simple函数继续对其进行预训练1个周期。这样做可以无缝地继续之前的训练过程，从而提高模型的性能和适应性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cell)",
   "language": "python",
   "name": "cell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
